{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wait Wait, Don't Analyze Me!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NPR logo](https://media.npr.org/branding/programs/wait-wait-dont-tell-me/branding_main-c5920a167d6a5d445ce86fac30b90454223b6b57.png \"One nerd's attempt to learn everything there is to know about NPR's greatest quiz show.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "[Wait Wait, Don't Tell Me!](https://www.npr.org/programs/wait-wait-dont-tell-me/) is NPR's longest-running news quiz show. Contestents call in to answer questions about the week's news, and a rotating cast of three panelists make jokes and parody newsworthy (and not-so-newsworthy) current events. Listening to \"Wait wait\" has been a highlight of my week since I was a kid, and it remains one of NPR's most popular segments. So what better way to show my appreciation than to take it apart and see what makes it tick?\n",
    "\n",
    "For this project, I have pulled text transcripts of each episode of \"Wait, Wait\", storing them as a MySQL library. I have two goals:\n",
    "1. Understand and predict jokes in the program.\n",
    "2. Create a \"Wait wait\" transcript generator, so that I don't have to wait a whole week between episodes!\n",
    "\n",
    "In this section, I will create a transcript generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "* Data Processing\n",
    "    * 0.1 [Loading data](#data-loading)\n",
    "    * 0.2 [Example transcript](#data-example)\n",
    "    * 0.3 [Encoding transcripts](#data-encoding)\n",
    "    * 0.4 [Building a training set](#data-train)\n",
    "* RNN Modeling\n",
    "    * 1.1 [Model Architecture](#model-initialize)\n",
    "    * 1.2 [Model training]()\n",
    "* Markov Model\n",
    "    * 2.1 [Markov Chain](#markov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 0: Initial data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Loading the data <a name=\"loading\"></a>\n",
    "Before I can analyze the data, I must first load it and process it. To accomplish this, I wrote a simple function to load in text files containing the transcripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries I'll be using\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import mysql.connector\n",
    "import re\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "import time\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# change the default font size in figures to be larger\n",
    "font = {'size'   : 15}\n",
    "plt.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the database of wait wait don't tell me transcripts\n",
    "cnx = mysql.connector.connect(database='wait_wait',\n",
    "                              user='root')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to pull some transcripts from the database\n",
    "def pull_transcript(n=5):\n",
    "    # instantiate a cursor to select data from the database\n",
    "    curs = cnx.cursor()\n",
    "    curs.execute(f'select * from transcripts limit {n}')\n",
    "    \n",
    "    # pull the data and convert to a pandas dataframe\n",
    "    df = pd.DataFrame(data = np.array(curs.fetchmany(n)),columns=curs.column_names)\n",
    "    df = df.set_index('id')\n",
    "    \n",
    "    # close the cursor\n",
    "    curs.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and pull all of the transcripts from the database - this dataset happens to be small enough that I can load it all at once.\n",
    "\n",
    "I also divide the transcripts randomly into testing, training, and validation sets. This will ensure that when I perform analyses, I don't build models that over-fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_transcripts = 4131\n",
    "transcript_df = pull_transcript(n=num_transcripts)\n",
    "\n",
    "# split the tables into testing and training sets, so that we don't over-fit. \n",
    "np.random.seed(42) # Ensures that the split is the same each round\n",
    "transcript_df['train'] = np.random.rand(num_transcripts)>.2\n",
    "transcript_df['test'] = transcript_df['train']==False\n",
    "\n",
    "# Further separate the training dataset into a training and validation set\n",
    "transcript_df['val'] = (np.random.rand(num_transcripts)>.8) & (transcript_df['train'])\n",
    "\n",
    "# ensure that the training and validation sets don't overlap\n",
    "transcript_df['train'] = transcript_df['train'] & (transcript_df['val']==False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>aired_at</th>\n",
       "      <th>url</th>\n",
       "      <th>segment</th>\n",
       "      <th>transcript</th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "      <th>val</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-05-04</td>\n",
       "      <td>https://www.npr.org/templates/transcript/trans...</td>\n",
       "      <td>who</td>\n",
       "      <td>\\n    \\n        \\n    \\n\\n    BILL KURTIS: Fro...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-05-04</td>\n",
       "      <td>https://www.npr.org/templates/transcript/trans...</td>\n",
       "      <td>panel</td>\n",
       "      <td>\\n    \\n        \\n    \\n\\n    PETER SAGAL, HOS...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-05-04</td>\n",
       "      <td>https://www.npr.org/templates/transcript/trans...</td>\n",
       "      <td>bluff</td>\n",
       "      <td>\\n    \\n        \\n    \\n\\n    BILL KURTIS: Fro...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-05-04</td>\n",
       "      <td>https://www.npr.org/templates/transcript/trans...</td>\n",
       "      <td>job</td>\n",
       "      <td>\\n    \\n        \\n    \\n\\n    PETER SAGAL, HOS...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-05-04</td>\n",
       "      <td>https://www.npr.org/templates/transcript/trans...</td>\n",
       "      <td>panel</td>\n",
       "      <td>\\n    \\n        \\n    \\n\\n    PETER SAGAL, HOS...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-05-04</td>\n",
       "      <td>https://www.npr.org/templates/transcript/trans...</td>\n",
       "      <td>limerick</td>\n",
       "      <td>\\n    \\n        \\n    \\n\\n    PETER SAGAL, HOS...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-05-04</td>\n",
       "      <td>https://www.npr.org/templates/transcript/trans...</td>\n",
       "      <td>lightning</td>\n",
       "      <td>\\n    \\n        \\n    \\n\\n    PETER SAGAL, HOS...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-05-04</td>\n",
       "      <td>https://www.npr.org/templates/transcript/trans...</td>\n",
       "      <td>predictions</td>\n",
       "      <td>\\n    \\n        \\n    \\n\\n    PETER SAGAL, HOS...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-04-27</td>\n",
       "      <td>https://www.npr.org/templates/transcript/trans...</td>\n",
       "      <td>who</td>\n",
       "      <td>\\n    \\n        \\n    \\n\\n    BILL KURTIS: Fro...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-04-27</td>\n",
       "      <td>https://www.npr.org/templates/transcript/trans...</td>\n",
       "      <td>panel</td>\n",
       "      <td>\\n    \\n        \\n    \\n\\n    PETER SAGAL, HOS...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode_id    aired_at                                                url  \\\n",
       "id                                                                             \n",
       "1           1  2019-05-04  https://www.npr.org/templates/transcript/trans...   \n",
       "2           1  2019-05-04  https://www.npr.org/templates/transcript/trans...   \n",
       "3           1  2019-05-04  https://www.npr.org/templates/transcript/trans...   \n",
       "4           1  2019-05-04  https://www.npr.org/templates/transcript/trans...   \n",
       "5           1  2019-05-04  https://www.npr.org/templates/transcript/trans...   \n",
       "6           1  2019-05-04  https://www.npr.org/templates/transcript/trans...   \n",
       "7           1  2019-05-04  https://www.npr.org/templates/transcript/trans...   \n",
       "8           1  2019-05-04  https://www.npr.org/templates/transcript/trans...   \n",
       "9           2  2019-04-27  https://www.npr.org/templates/transcript/trans...   \n",
       "10          2  2019-04-27  https://www.npr.org/templates/transcript/trans...   \n",
       "\n",
       "        segment                                         transcript  train  \\\n",
       "id                                                                          \n",
       "1           who  \\n    \\n        \\n    \\n\\n    BILL KURTIS: Fro...   True   \n",
       "2         panel  \\n    \\n        \\n    \\n\\n    PETER SAGAL, HOS...   True   \n",
       "3         bluff  \\n    \\n        \\n    \\n\\n    BILL KURTIS: Fro...   True   \n",
       "4           job  \\n    \\n        \\n    \\n\\n    PETER SAGAL, HOS...   True   \n",
       "5         panel  \\n    \\n        \\n    \\n\\n    PETER SAGAL, HOS...  False   \n",
       "6      limerick  \\n    \\n        \\n    \\n\\n    PETER SAGAL, HOS...  False   \n",
       "7     lightning  \\n    \\n        \\n    \\n\\n    PETER SAGAL, HOS...  False   \n",
       "8   predictions  \\n    \\n        \\n    \\n\\n    PETER SAGAL, HOS...   True   \n",
       "9           who  \\n    \\n        \\n    \\n\\n    BILL KURTIS: Fro...   True   \n",
       "10        panel  \\n    \\n        \\n    \\n\\n    PETER SAGAL, HOS...   True   \n",
       "\n",
       "     test    val  \n",
       "id                \n",
       "1   False  False  \n",
       "2   False  False  \n",
       "3   False  False  \n",
       "4   False  False  \n",
       "5    True  False  \n",
       "6    True  False  \n",
       "7    True  False  \n",
       "8   False  False  \n",
       "9   False  False  \n",
       "10  False  False  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity, I'll set all letters to lower-case\n",
    "transcript_df.loc[:,'transcript'] = transcript_df.loc[:,'transcript'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Example transcript<a name=\"data-example\"></a>\n",
    "\n",
    "To understand the data, it helps to first see what the raw data looks like. Let's print a little bit of the transcript from the first dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    \n",
      "        \n",
      "    \n",
      "\n",
      "    bill kurtis: from npr and wbez chicago, this is wait wait... don't tell me, the npr news quiz. hey, arthur miller - step into this cruci-bill (ph).\n",
      "    (laughter)\n",
      "    kurtis: i'm bill kurtis. and here's your host at the chase bank auditorium in downtown chicago, peter sagal.\n",
      "    peter sagal, host: \n",
      "    thank you, bill. thank you, everybody.\n",
      "    (cheering)\n",
      "    sagal: thank you so much. we have a very interesting show for you today. later on, we're going to be talking to m\n"
     ]
    }
   ],
   "source": [
    "print(transcript_df.loc[1,'transcript'][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    \\n        \\n    \\n\\n    bill kurtis: from npr and wbez chicago, this is wait wait... don't tell me, the npr news quiz. hey, arthur miller - step into this cruci-bill (ph).\\n    (laughter)\\n    kurtis: i'm bill kurtis. and here's your host at the chase bank auditorium in downtown chicago, peter sagal.\\n    peter sagal, host: \\n    thank you, bill. thank you, everybody.\\n    (cheering)\\n    sagal: thank you so much. we have a very interesting show for you today. later on, we're going to be talking to microsoft co-founder steve ballmer. he is, we believe, the richest guest we've ever had. but, of course, your true wealth is measured in your friends. and this just in - he has more friends, too.\\n    (laughter)\\n    sagal: but first, as many of you know, the npr podcast feeds got all screwed up last week. people who tried to download our show got, for example, how i built this instead, for which i apologize. and the people who wanted how i built this got us, for which i apologize even more.\\n    (l\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_df.loc[1,'transcript'][:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, we can note a number of features. First, audience responses are noted with the '(LAUGHTER)' marker and '(APPLAUSE)' marker. This will prove very useful, as we have an automatic metric for \"funniness\" of the preceding text. \n",
    "\n",
    "Speakers' names are in all caps, followed by a colon. Speakers are also separated by a line break and a tab, which could potentially be used to segment the text into phrases by various people. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3 Encoding the text <a name='data-encoding'></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be building a letter-based generator for now, so I want a way to encode both letters and punctuation as integers (eventually, to be transferred into a one-hot encoding scheme for transferring to the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset includes 77 unique tokens\n"
     ]
    }
   ],
   "source": [
    "all_tokens = set(transcript_df.loc[transcript_df.train,'transcript'].str.cat())\n",
    "print(f'The dataset includes {len(all_tokens)} unique tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary converting letters/punctuation to integers\n",
    "conversion_dict = {}\n",
    "for i, token in enumerate(all_tokens):\n",
    "    conversion_dict[token] = i\n",
    "    \n",
    "# Make a second dictionary to go in the other direction\n",
    "reversion_dict = dict( (v,k) for k, v in conversion_dict.items() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple function to encode transcript\n",
    "def encode_transcript(transcript):\n",
    "    return [conversion_dict.get(n,len(all_tokens)) for n in transcript]\n",
    "def decode_transcript(transcript):\n",
    "    return [reversion_dict.get(n,len(all_tokens)) for n in transcript]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode each of the transcripts by converting letters and punctuation to integers\n",
    "transcript_df['encoded'] = transcript_df.loc[:,'transcript'].apply(encode_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>aired_at</th>\n",
       "      <th>url</th>\n",
       "      <th>segment</th>\n",
       "      <th>transcript</th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "      <th>val</th>\n",
       "      <th>encoded</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-05-04</td>\n",
       "      <td>https://www.npr.org/templates/transcript/trans...</td>\n",
       "      <td>who</td>\n",
       "      <td>\\n    \\n        \\n    \\n\\n    bill kurtis: fro...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[44, 1, 1, 1, 1, 44, 1, 1, 1, 1, 1, 1, 1, 1, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-05-04</td>\n",
       "      <td>https://www.npr.org/templates/transcript/trans...</td>\n",
       "      <td>panel</td>\n",
       "      <td>\\n    \\n        \\n    \\n\\n    peter sagal, hos...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[44, 1, 1, 1, 1, 44, 1, 1, 1, 1, 1, 1, 1, 1, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-05-04</td>\n",
       "      <td>https://www.npr.org/templates/transcript/trans...</td>\n",
       "      <td>bluff</td>\n",
       "      <td>\\n    \\n        \\n    \\n\\n    bill kurtis: fro...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[44, 1, 1, 1, 1, 44, 1, 1, 1, 1, 1, 1, 1, 1, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-05-04</td>\n",
       "      <td>https://www.npr.org/templates/transcript/trans...</td>\n",
       "      <td>job</td>\n",
       "      <td>\\n    \\n        \\n    \\n\\n    peter sagal, hos...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[44, 1, 1, 1, 1, 44, 1, 1, 1, 1, 1, 1, 1, 1, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-05-04</td>\n",
       "      <td>https://www.npr.org/templates/transcript/trans...</td>\n",
       "      <td>panel</td>\n",
       "      <td>\\n    \\n        \\n    \\n\\n    peter sagal, hos...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[44, 1, 1, 1, 1, 44, 1, 1, 1, 1, 1, 1, 1, 1, 4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode_id    aired_at                                                url  \\\n",
       "id                                                                             \n",
       "1           1  2019-05-04  https://www.npr.org/templates/transcript/trans...   \n",
       "2           1  2019-05-04  https://www.npr.org/templates/transcript/trans...   \n",
       "3           1  2019-05-04  https://www.npr.org/templates/transcript/trans...   \n",
       "4           1  2019-05-04  https://www.npr.org/templates/transcript/trans...   \n",
       "5           1  2019-05-04  https://www.npr.org/templates/transcript/trans...   \n",
       "\n",
       "   segment                                         transcript  train   test  \\\n",
       "id                                                                            \n",
       "1      who  \\n    \\n        \\n    \\n\\n    bill kurtis: fro...   True  False   \n",
       "2    panel  \\n    \\n        \\n    \\n\\n    peter sagal, hos...   True  False   \n",
       "3    bluff  \\n    \\n        \\n    \\n\\n    bill kurtis: fro...   True  False   \n",
       "4      job  \\n    \\n        \\n    \\n\\n    peter sagal, hos...   True  False   \n",
       "5    panel  \\n    \\n        \\n    \\n\\n    peter sagal, hos...  False   True   \n",
       "\n",
       "      val                                            encoded  \n",
       "id                                                            \n",
       "1   False  [44, 1, 1, 1, 1, 44, 1, 1, 1, 1, 1, 1, 1, 1, 4...  \n",
       "2   False  [44, 1, 1, 1, 1, 44, 1, 1, 1, 1, 1, 1, 1, 1, 4...  \n",
       "3   False  [44, 1, 1, 1, 1, 44, 1, 1, 1, 1, 1, 1, 1, 1, 4...  \n",
       "4   False  [44, 1, 1, 1, 1, 44, 1, 1, 1, 1, 1, 1, 1, 1, 4...  \n",
       "5   False  [44, 1, 1, 1, 1, 44, 1, 1, 1, 1, 1, 1, 1, 1, 4...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features=None, categories='auto', drop=None,\n",
       "              dtype=<class 'numpy.float64'>, handle_unknown='error',\n",
       "              n_values=None, sparse=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a one-hot encoder to finally yield data in a one-hot version\n",
    "onehotencoder = OneHotEncoder(categories='auto',sparse=False)\n",
    "onehotencoder.fit(np.concatenate(transcript_df.encoded.values).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to apply the one hot scheme to the integer-encoded values\n",
    "def one_hot_transcript(transcript):\n",
    "    integer_transcript = np.array(encode_transcript(transcript)).reshape(-1,1)\n",
    "    return onehotencoder.transform(integer_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the encoded transcript values to the one-hot encodings\n",
    "transcript_df['encoded'] = transcript_df.loc[:,'transcript'].apply(one_hot_transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.4 Building a training set <a name='data-train'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build up the training set, we will be taking all of our training transcripts and breaking them up into pieces of a set size. The \"x\" values will be the set of encoded integers, and the \"y\" value will be the integer that immediately follows. The goal of the model will be to predict the next letter (or punctuation mark), given the previous letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "n_times = 50\n",
    "n_components = len(all_tokens)+1\n",
    "step_size = 50\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_data(transcript):\n",
    "    # makes calculating size easier to pre-build the iterator\n",
    "    iterator = range(0,transcript.shape[0]-step_size-n_times,step_size) \n",
    "    \n",
    "    # calculate the size of data we will be generating\n",
    "    n_examples = len(iterator)\n",
    "\n",
    "    # initialize x and y values\n",
    "    x = np.zeros([n_examples,n_times,n_components])\n",
    "    y = np.zeros([n_examples,n_components])\n",
    "\n",
    "    # fill in the values for each split\n",
    "    for step,startpos in enumerate(iterator):\n",
    "        x[step] = transcript[startpos:startpos+n_times,:]\n",
    "        y[step] = transcript[startpos+n_times,:]\n",
    "    \n",
    "    # return x and y\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For each element of the training transcript set, generate training data\n",
    "def combine_model_data(transcript_df):\n",
    "    x = list()\n",
    "    y = list()\n",
    "    for i,transcript in enumerate(transcript_df.encoded):\n",
    "        x_,y_ = generate_model_data(transcript)\n",
    "        x.append(x_)\n",
    "        y.append(y_)\n",
    "\n",
    "        # report progress\n",
    "        if i%50==0:\n",
    "            print(i)\n",
    "    \n",
    "    # combine all sets into arrays\n",
    "    x = np.concatenate(x,axis=0)\n",
    "    y = np.concatenate(y,axis=0)\n",
    "        \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "# keeping things small for now, to ensure my code base works before going in with everything\n",
    "# x_train,y_train = combine_model_data(transcript_df.loc[:500,:])\n",
    "x_val,y_val     = combine_model_data(transcript_df.loc[1000:1200,  :])\n",
    "# x_test,y_test   = combine_model_data(transcript_df.loc[120:140, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.5 Data Generator\n",
    "This is a different way of making the data. Doing the massive conversion of transcript to one-hot encoding is pretty memory-intensive. One way to improve that is to only process some of the data at a time, using a generator. This could also enable me to add some additional randomization into my process, so I'm going to play around with it. This code is closely based on the tutorial here: https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    # Generates data for Keras\n",
    "    def __init__(self, df, batch_size=32,n_times=n_times,n_components=n_components,shuffle=True):\n",
    "        # Initialization\n",
    "        self.batch_size = batch_size\n",
    "        self.df = df.encoded\n",
    "        self.indices = np.copy(self.df.index.values)\n",
    "        self.dim = (batch_size,n_times,n_components)\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        # Denotes the number of batches per epoch\n",
    "        return len(self.df.index.values)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(self.indices[index])\n",
    "\n",
    "        return X, y\n",
    "\n",
    "#     def on_epoch_end(self):\n",
    "#         #'Updates indexes after each epoch'\n",
    "#         self.indices = self.df.index.values\n",
    "#         if self.shuffle == True:\n",
    "#             np.random.shuffle(self.indices)\n",
    "\n",
    "    def __data_generation(self, transcript_num):\n",
    "        #'Generates data containing batch_size samples' \n",
    "        # Initialization\n",
    "        X = np.empty(self.dim)\n",
    "        y = np.empty((self.batch_size,self.dim[-1]), dtype=int)\n",
    "\n",
    "        # Pull batch_size random samples from the specified transcript\n",
    "        indices = np.random.randint(low=0,\n",
    "                                    high=self.df[transcript_num].shape[0]-101,\n",
    "                                    size=self.batch_size)\n",
    "        \n",
    "        # for each start position, pull a sequence of 100 chars for X, and the 101'st char for y\n",
    "        for i, startpos in enumerate(indices):\n",
    "            # Store sample\n",
    "            X[i,] = self.df[transcript_num][i:i+100,:]\n",
    "\n",
    "            # Store class\n",
    "            y[i] = self.df[transcript_num][i+100,:]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = DataGenerator(transcript_df.loc[transcript_df.train,:],shuffle=True)\n",
    "val_gen = DataGenerator(transcript_df.loc[transcript_df.val,:],shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Building and Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Generate transcript function\n",
    "First, specify a function that, given a model and a sample of text, will generate the next text for the transcript. We'll run this function occasionally in order to visualize how our model learning is progressing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predict_transcript(sample,chars_to_generate=50):\n",
    "    sample = sample.reshape(1,sample.shape[0],sample.shape[1])\n",
    "    \n",
    "    # initialize the phrase\n",
    "    predicted_phrase = np.concatenate((sample,np.zeros((1,chars_to_generate,n_components))),axis=1)\n",
    "\n",
    "    for char in range(n_times,n_times+chars_to_generate):\n",
    "        # calculate the probability distribution\n",
    "        probability = model.predict(sample).squeeze()\n",
    "\n",
    "        # guess the next character and add it to our prediction\n",
    "        guess = np.random.choice(a=n_components,p=probability)\n",
    "        predicted_phrase[0,char,guess] = 1\n",
    "\n",
    "        # feed the results into the next step of the model\n",
    "        sample = predicted_phrase[:,(char+1-n_times):char+1,:]\n",
    "\n",
    "    # Convert to text\n",
    "    predicted_phrase = ''.join(decode_transcript(np.argmax(predicted_phrase,axis=-1).squeeze()))\n",
    "    \n",
    "    # Print result\n",
    "    print(predicted_phrase)\n",
    "    \n",
    "    # output the phrase\n",
    "    return predicted_phrase\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Specifying model architecture <a name='model-initialize'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to use a recurrent neural network (GRU) to generate my text. There are other systems I would also like to play with in the future (e.g. Markov models, transformer network), but this will make a good start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential models are pretty simple, I'll start there.\n",
    "from keras.models import Model\n",
    "from keras.layers import GRU,Dense,Bidirectional,Input,Add,Dropout,SpatialDropout1D\n",
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter setting\n",
    "n_units = 256 # size of RNN layers\n",
    "\n",
    "# helper function to add in a middle GRU layer and sum the outputs+inputs to that layer\n",
    "def Add_GRU_layer(inp,n_units):\n",
    "    mid = Bidirectional(GRU(n_units,activation='tanh',input_shape=(n_times,n_components),return_sequences=True))(inp)\n",
    "    mid = SpatialDropout1D(rate=.25)(mid)\n",
    "    final = Add()([inp,mid])\n",
    "    return final\n",
    "\n",
    "## Model layer definitions start here\n",
    "x_in = Input(shape=(None,n_components))\n",
    "\n",
    "# GRU layers\n",
    "x = Bidirectional(GRU(n_units,activation='tanh',input_shape=(n_times,n_components),return_sequences=True))(x_in)\n",
    "\n",
    "# middle layers, with skip-gram combinations\n",
    "for mid_layer in range(1):\n",
    "    x = Add_GRU_layer(x,n_units)\n",
    "\n",
    "# final layer, no returning sequences\n",
    "x = Bidirectional(GRU(256,activation='tanh',input_shape=(n_times,n_components),return_sequences=False))(x)\n",
    "\n",
    "\n",
    "dense_out = Dense(n_components,activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=[x_in],outputs=[dense_out])\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_17 (InputLayer)           (None, None, 78)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_70 (Bidirectional (None, None, 512)    514560      input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_71 (Bidirectional (None, None, 512)    1181184     bidirectional_70[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_5 (SpatialDro (None, None, 512)    0           bidirectional_71[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_40 (Add)                    (None, None, 512)    0           bidirectional_70[0][0]           \n",
      "                                                                 spatial_dropout1d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_72 (Bidirectional (None, 512)          1181184     add_40[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 78)           40014       bidirectional_72[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 2,916,942\n",
      "Trainable params: 2,916,942\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Generate a model summary to visualize the layers and model size\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback to generate an example transcript prediction each time\n",
    "class generator_callback(Callback):\n",
    "    \n",
    "    def on_epoch_end(self,epoch=0,logs={}):\n",
    "        Predict_transcript(x_val[100]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "82/82 [==============================] - 73s 889ms/step - loss: 0.6167\n",
      "Epoch 2/10\n",
      "82/82 [==============================] - 73s 892ms/step - loss: 0.8505\n",
      "Epoch 3/10\n",
      "82/82 [==============================] - 72s 879ms/step - loss: 0.8817\n",
      "Epoch 4/10\n",
      "82/82 [==============================] - 70s 859ms/step - loss: 0.8362\n",
      "Epoch 5/10\n",
      "82/82 [==============================] - 76s 929ms/step - loss: 0.6920\n",
      "Epoch 6/10\n",
      "82/82 [==============================] - 78s 954ms/step - loss: 0.7318\n",
      "Epoch 7/10\n",
      "82/82 [==============================] - 77s 938ms/step - loss: 0.7763\n",
      "Epoch 8/10\n",
      "82/82 [==============================] - 74s 896ms/step - loss: 0.7679\n",
      "Epoch 9/10\n",
      "82/82 [==============================] - 72s 876ms/step - loss: 0.6755\n",
      "Epoch 10/10\n",
      "82/82 [==============================] - 74s 899ms/step - loss: 0.6784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x8bb10c438>]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss',patience=8)\n",
    "progress_reporter = generator_callback()\n",
    "history = model.fit_generator(train_gen,\n",
    "                              epochs=10,\n",
    "                              steps_per_epoch=82)\n",
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a speeding problem. what to do? beg police to issue tickets? install speed bumps? or you could just this whas ding? e cia abaut sthings sirging afim \n"
     ]
    }
   ],
   "source": [
    "Predict_transcript(x_val[100],chars_to_generate=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Markov Chain <a name=\"markov\"></a>\n",
    "As a control, let's build a much simpler system, called a \"Markov chain.\" This system simply calculates the probability of a next character, given the previous characters (how many previous characters is a selectable parameter). It then generates text by sampling this probability matrix. We can compare the performance of the Markov chain with the performance of the RNN.\n",
    "\n",
    "This code initialized with the tutorial provided here: https://eli.thegreenplace.net/2018/elegant-python-code-for-a-markov-chain-text-generator/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "\n",
    "# This is the length of the \"state\" the current character is predicted from.\n",
    "STATE_LEN = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join all transcripts together\n",
    "data = transcript_df.loc[:,'transcript'].str.cat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning model...\n",
      "0/31012208\n",
      "10000000/31012208\n",
      "20000000/31012208\n",
      "30000000/31012208\n"
     ]
    }
   ],
   "source": [
    "# initialize model\n",
    "markovmodel = defaultdict(Counter)\n",
    "print('Learning model...')\n",
    "for i in range(len(data) - STATE_LEN):\n",
    "    state = data[i:i + STATE_LEN]\n",
    "    nxt = data[i + STATE_LEN]\n",
    "    markovmodel[state][nxt] += 1\n",
    "    if i%10000000 ==0:\n",
    "        print(f'{i}/{len(data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling...\n",
      "e me a break people, do i need to even hear any of the other sex. it's a great, much needed resource, except when you get your car, you walk away.\n",
      "    (applause)\n",
      "    sagal: i don't remember anything?\n",
      "    kluwe: yeah, actually we do most of our playing during the interrogation after they told her they would spend a head-banging night in jail.\n",
      "    pesca: yeah, could you tap dance?\n",
      "    foster: pretty good, peter. she got two right...\n",
      "    clayton: a company in blank.\n",
      "    bodett: all right, a local.\n",
      "    (applause)\n",
      "    poundstone: you know, i don't - i want - if anyone is going to do what?\n",
      "    o'rourke: tax.\n",
      "    sagal: right.\n",
      "    (soundbite of laughter)\n",
      "    sagal: this may be too personal but do you have sex with animals?\n",
      "    sagal: no. after snakes were reported emerging from a chia lawn. you can pick the one you think is the real story of a deal discovered in the midterm elections by getting as far away from what?\n",
      "    luke burbank: who told her about the identities of his birth parents. he may have thought that counts.\n",
      "    (applause)\n",
      "    sagal: so, there you go.\n",
      "    (laughter)\n",
      "    sagal: bill, how did whitney do on our quiz?\n",
      "    kurtis: nine-one-one? i think you walk through this wall like harry potter books out loud and i loved the fbi tour. and this one particular place in chinese culture, places where people come to the end of my index finger.\n",
      "    babylon: let's go.\n",
      "    sagal: cheese.\n",
      "    (soundbite of laughter)\n",
      "    sagal: well done. all right, mike, your first night in space could you - how could you...\n",
      "    sagal: you're going to start us off with who's bill this time. bill kurtis is going to read you three quotes from this week's news, so give us a call at 1-888-wait-wait, that's 1-888-924-8924. now let's welcome our first listener contestant. hi, you are on wait wait...don't tell me, the npr news quiz. i'm carl kasell. we're playing this week with mo rocca, kyrie o'connor, adam felber and amy dickinson.\n",
      "    amy dickinson: yeah...\n",
      "    sagal: yeah. paula, do you mean like you sing your instructions to kill barney the dinosaur, right?\n",
      "    curtis: yes, it's called let's move, right?\n",
      "    kaling: yeah.\n",
      "    felber: oh, his parents wouldn't let her swim back to the orangutans for punishment there. rick schwartz is the official russian news agency, he still had time, that guy, to do what last week?\n",
      "    (laughter)\n",
      "    papa: ...and can be used as a beer cooler until you die.\n",
      "    (laughter)\n",
      "    sagal: but i don't buy that. and i think we're like, that's what arnold said.\n",
      "    (soundbite of laughter)\n",
      "    sagal: now i want to make.\n",
      "    goldblum: yeah, i know...\n",
      "    sagal: right.\n",
      "    (laughter)\n",
      "    hamm: as a fictional advertising executives with all their stuff.\n",
      "    (laughter)\n",
      "    pesca: indiana police suspected a woman might be.\n",
      "    schwartz: yeah.\n",
      "    sagal: yes indeed, a grizzly.\n",
      "    (soundbite of laughter)\n",
      "    rocca: yes, i love those.\n",
      "    m: oh, thomas jefferson that ben franklin was referring to my dad and if you just keep showing up, you know - my dad told me one time somebody gets murdered in the church.\n",
      "    (soundbite of laughter)\n",
      "    sagal: b: he walked past, zuma did, an identical chess set given to the queen by nelson mandela.\n",
      "    sagal: answer two out of three questions about the famous singer songwriters like yourself. like, there's a wounded bird quality that i do think - i will say as a woman that i did also find appealing.\n",
      "    brennan: there are three of you?\n",
      "    ben folds: we thought it was the five-second rule, that you have solved the problem of all the containers come with clip-on backdrops with a variety of creative modifications. the velvet is shredded on the pricey orchestra seats.\n",
      "    (laughter)\n",
      "    sagal: i thought it would be easier. now it's plateau biking. that great romantic life yourself?\n",
      "    (laughter)\n",
      "    (soundbite of applause)\n",
      "    sagal: from jessi klein, tom bodett and helen hong. and here again is your host at the chase bank auditorium in downtown chicago, peter sagal.\n",
      "    peter sagal, host: \n",
      "    we have a tie for first place, peter. faith had eight correct answers, peter, and that was canned mushrooms. they didn't have anything...\n",
      "    sagal: no.\n",
      "    (laughter)\n",
      "    sagal: this week, the big news this week came out that it's 5 percent actual hair, 45 percent polyester, and the rest of the time sitting in a storage closet at a temple..\n",
      "    (laughter)\n",
      "    pierce: a local beer parlor.\n",
      "    host: no.\n",
      "    o'connor: but they're working, but it's, you know, it wasn't for flashbacks, i wouldn't have the baby sliding all around. you don't know the physics.\n",
      "    (soundbite of bell)\n",
      "    (soundbite of laughter)\n",
      "    sagal: do you really?\n",
      "    shields: oh, it's all suggested, but there's nothing creepy about about a uniform that you wear every day to work.\n",
      "    higgins: christina aguilera to roseanne barr impersonator.\n",
      "    sagal: oh, how are things in orlando?\n",
      "    martinez: bill clinton.\n",
      "    (laughter, applause)\n",
      "    host: remember them from that machine with the claw where it dips down.\n",
      "    (laughter)\n",
      "    sagal: i mean, it's an expensive pet?\n",
      "    case: she's just - she's just like i was. the only thing people knew about san diego was the san diego civic theatre. thanks to everybody at kunc. and thanks to all of you for listening. i am peter sagal. we'll see you next week in millennium park in chicago.\n",
      "    sagal: absolutely.\n",
      "    bodden: let me tell you that support for npr comes from npr stations and lumber liquidators, a proud sponsor of npr, offering more than 400 styles, including house cleaners, handymen, landscapers and remodelers. learn more at carmax.com. the kaufman foundation, committed to doing its part to make the world go round?\n",
      "    (laughter)\n",
      "    sagal: so what do you do there in eugene?\n",
      "    rennick: yeah, absolutely.\n",
      "    sagal: indeed.\n",
      "    basalyga: i am not. i'm an import.\n",
      "    sagal: right.\n",
      "    (soundbite of laughter)\n",
      "    ax: because...\n",
      "    brownstein: yeah.\n",
      "    sagal: well it sent its pacifier through the x-ray.\n",
      "    pierce: that sounds so fun.\n",
      "    sagal: oh, it's terrible here. i wish i was never born.\n",
      "    (laughter)\n",
      "    sagal: i've always wanted. this week our panelists are going to tell you this, luke, but they already do.\n",
      "    poundstone: i do that when we accidentally shooting himself while bowling.\n",
      "    organizers announced this week that despite what you might think to yourself, farmers only, that sounds lovely.\n",
      "    sagal: i like the hamburgers is terrific.\n",
      "    oldman: i don't know. my - i - really, education has done good things for me.\n",
      "    sagal: how hard has it been tough being - having being named king? have people expected regal things from you because you want to go. but some elevators around the world are cancer and the show \"entourage.\"\n",
      "    (laughter)\n",
      "    oswalt: because i was going to say jelly doughnuts. on tuesday, republican debate.\n",
      "    (laughter)\n",
      "    armisen: they have no effect. is that true?\n",
      "    sonnenfeld: and then - well, with my fist. it can happen.\n",
      "    sagal: \"the fact these men spend all this time with artists you've never heard before in the world, but it means something very common that carl started doing it at age 11. which would be really surprised. you know, we had pens that fired, you know, where i'm from.\n",
      "    (laughter)\n",
      "    felber: i would argue that, yes.\n",
      "    sagal: hi, you're on wait wait... don't tell me, the npr news quiz. i'm carl kasell, and here's your host, at the chase bank auditorium in chicago, ill. for tickets and more information, go to waitwait.npr.org. there, you can find out about attending our weekly live shows back at the chase bank auditorium in chicago, illinois. for tickets or more information, go to wbez.org or you can find a link at our website waitwait.npr.org. right now, it's time to move on to the game, we have a question from the government open. it was an amazing entrepreneur who wants to reduce stress and improves memory.\n",
      "    babylon: preach that. preach.\n",
      "    grosz: wow.\n",
      "    pesca: and the cross-country skiing at the 2018 olympics in sochi, let us remember that when the show was young and immature and they steal the signs. quote, \"she's an artist, an entrepreneur, ra-men offers you a steaming bowl of ramen noodles with a special priority to gypsies, tramps and thieves.\n",
      "    (laughter)\n",
      "    sagal: orange county, california and everybody didn't like it? oh, those were three plays you had with the rams, right?\n",
      "    sagal: yeah.\n",
      "    (soundbite of bell)\n",
      "    sagal: brian what?\n",
      "    (laughter)\n",
      "    grosz: dr. sakai's method involves asking his patients to identify the suspect in a photo because he was busy trying to teach a pigeon a trick.\n",
      "    (laughter)\n",
      "    sagal: bill, how did jeff do?\n",
      "    kasell: super congress. and since i learned the lesson.\n",
      "    : sure.\n",
      "    (soundbite of bell ringing)\n",
      "    bodett: oh, god. he dressed as a mannequin and in real life though, who knows? maybe it'll catch on, even in the dark. early testers have noticed what people are coming up with that is that if the cat did it, it would frame the dog.\n",
      "    (soundbite of laughter)\n",
      "    sagal: rebecca's mother said she wanted to keep up with all of them.\n",
      "    (laughter)\n",
      "    bouton: hey, look at me. i just hit a home run.\n",
      "    sagal: right. \n",
      "    (soundbite of applause)\n",
      "    sagal: next, it's a veteran rate of \"late night with david letterman's original sidekick?\n",
      "    wainwright: oh, that's the problem here is that white castle? what is lower on the food totem pole?\n",
      "    quiett: i am.\n",
      "    sagal: i'm glad to hear it. charlottesville, va., headlining the boycott bigotry show. it's negin farsad.\n",
      "    negin farsad: (imitating british accent) mother dear, may i have a pull on the royal breast?\n",
      "    (laughter)\n",
      "    dempster: yeah, that is tough work.\n",
      "    sheldon: about 70 miles south of here right now.\n",
      "    sagal: first, let's hear from maz jobrani, in south korea's space-out competition, where two poets accuse each other of secretly satirizing the nation's dvrs recorded it anyway.\n",
      "    sagal: but first, you probably get compared to the rest of us.\n",
      "    (laughter)\n",
      "    sagal: the trump inaugural committee. yes\n"
     ]
    }
   ],
   "source": [
    "print('Sampling...')\n",
    "state = random.choice(list(markovmodel))\n",
    "out = state\n",
    "for i in range(10000):\n",
    "    out += random.choices(list(markovmodel[state]), markovmodel[state].values())[0]\n",
    "    state = out[-STATE_LEN:]\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_36_env",
   "language": "python",
   "name": "py_36_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
