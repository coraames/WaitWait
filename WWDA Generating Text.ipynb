{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wait Wait, Don't Analyze Me!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NPR logo](https://media.npr.org/branding/programs/wait-wait-dont-tell-me/branding_main-c5920a167d6a5d445ce86fac30b90454223b6b57.png \"One nerd's attempt to learn everything there is to know about NPR's greatest quiz show.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "[Wait Wait, Don't Tell Me!](https://www.npr.org/programs/wait-wait-dont-tell-me/) is NPR's longest-running news quiz show. Contestents call in to answer questions about the week's news, and a rotating cast of three panelists make jokes and parody newsworthy (and not-so-newsworthy) current events. Listening to \"Wait wait\" has been a highlight of my week since I was a kid, and it remains one of NPR's most popular segments. So what better way to show my appreciation than to take it apart and see what makes it tick?\n",
    "\n",
    "For this project, I have pulled text transcripts of each episode of \"Wait, Wait\", storing them as a MySQL library. I have two goals:\n",
    "1. Understand and predict jokes in the program.\n",
    "2. Create a \"Wait wait\" transcript generator, so that I don't have to wait a whole week between episodes!\n",
    "\n",
    "In this section, I will create a transcript generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "* Data Processing\n",
    "    * 0.1 [Loading data](#data-loading)\n",
    "    * 0.2 [Example transcript](#data-example)\n",
    "    * 0.3 [Encoding transcripts](#data-encoding)\n",
    "    * 0.4 [Building a training set](#data-train)\n",
    "* Markov Model\n",
    "    * 1.1 [Markov Chain](#markov)\n",
    "* RNN Modeling\n",
    "    * 2.1 [Model Architecture](#rnn-initialize)\n",
    "    * 2.2 [Model training](#rnn-train)\n",
    "    * 2.3 [Model testing](#rnn-test)\n",
    "* Attention Network Modeling\n",
    "    * 3.1 [Model Architecture](#attn-initialize)\n",
    "    * 3.2 [Model training](#attn-train)\n",
    "    * 3.3 [Model testing](#attn-test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 0: Initial data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Loading the data <a name=\"loading\"></a>\n",
    "Before I can analyze the data, I must first load it and process it. To accomplish this, I wrote a simple function to load in text files containing the transcripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries I'll be using\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import mysql.connector\n",
    "import re\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "import time\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# change the default font size in figures to be larger\n",
    "font = {'size'   : 15}\n",
    "plt.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the database of wait wait don't tell me transcripts\n",
    "cnx = mysql.connector.connect(database='wait_wait',\n",
    "                              user='root')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to pull some transcripts from the database\n",
    "def pull_transcript(n=5):\n",
    "    # instantiate a cursor to select data from the database\n",
    "    curs = cnx.cursor()\n",
    "    curs.execute(f'select * from transcripts limit {n}')\n",
    "    \n",
    "    # pull the data and convert to a pandas dataframe\n",
    "    df = pd.DataFrame(data = np.array(curs.fetchmany(n)),columns=curs.column_names)\n",
    "    df = df.set_index('id')\n",
    "    \n",
    "    # close the cursor\n",
    "    curs.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and pull all of the transcripts from the database - this dataset happens to be small enough that I can load it all at once.\n",
    "\n",
    "I also divide the transcripts randomly into testing, training, and validation sets. This will ensure that when I perform analyses, I don't build models that over-fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_transcripts = 4131\n",
    "transcript_df = pull_transcript(n=num_transcripts)\n",
    "\n",
    "# split the tables into testing and training sets, so that we don't over-fit. \n",
    "np.random.seed(42) # Ensures that the split is the same each round\n",
    "transcript_df['train'] = np.random.rand(num_transcripts)>.2\n",
    "transcript_df['test'] = transcript_df['train']==False\n",
    "\n",
    "# Further separate the training dataset into a training and validation set\n",
    "transcript_df['val'] = (np.random.rand(num_transcripts)>.8) & (transcript_df['train'])\n",
    "\n",
    "# ensure that the training and validation sets don't overlap\n",
    "transcript_df['train'] = transcript_df['train'] & (transcript_df['val']==False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>aired_at</th>\n",
       "      <th>url</th>\n",
       "      <th>segment</th>\n",
       "      <th>transcript</th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "      <th>val</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-05-04</td>\n",
       "      <td>https://www.npr.org/templates/transcript/trans...</td>\n",
       "      <td>who</td>\n",
       "      <td>\\n    \\n        \\n    \\n\\n    BILL KURTIS: Fro...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-05-04</td>\n",
       "      <td>https://www.npr.org/templates/transcript/trans...</td>\n",
       "      <td>panel</td>\n",
       "      <td>\\n    \\n        \\n    \\n\\n    PETER SAGAL, HOS...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-05-04</td>\n",
       "      <td>https://www.npr.org/templates/transcript/trans...</td>\n",
       "      <td>bluff</td>\n",
       "      <td>\\n    \\n        \\n    \\n\\n    BILL KURTIS: Fro...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-05-04</td>\n",
       "      <td>https://www.npr.org/templates/transcript/trans...</td>\n",
       "      <td>job</td>\n",
       "      <td>\\n    \\n        \\n    \\n\\n    PETER SAGAL, HOS...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-05-04</td>\n",
       "      <td>https://www.npr.org/templates/transcript/trans...</td>\n",
       "      <td>panel</td>\n",
       "      <td>\\n    \\n        \\n    \\n\\n    PETER SAGAL, HOS...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-05-04</td>\n",
       "      <td>https://www.npr.org/templates/transcript/trans...</td>\n",
       "      <td>limerick</td>\n",
       "      <td>\\n    \\n        \\n    \\n\\n    PETER SAGAL, HOS...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-05-04</td>\n",
       "      <td>https://www.npr.org/templates/transcript/trans...</td>\n",
       "      <td>lightning</td>\n",
       "      <td>\\n    \\n        \\n    \\n\\n    PETER SAGAL, HOS...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-05-04</td>\n",
       "      <td>https://www.npr.org/templates/transcript/trans...</td>\n",
       "      <td>predictions</td>\n",
       "      <td>\\n    \\n        \\n    \\n\\n    PETER SAGAL, HOS...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-04-27</td>\n",
       "      <td>https://www.npr.org/templates/transcript/trans...</td>\n",
       "      <td>who</td>\n",
       "      <td>\\n    \\n        \\n    \\n\\n    BILL KURTIS: Fro...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-04-27</td>\n",
       "      <td>https://www.npr.org/templates/transcript/trans...</td>\n",
       "      <td>panel</td>\n",
       "      <td>\\n    \\n        \\n    \\n\\n    PETER SAGAL, HOS...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode_id    aired_at                                                url  \\\n",
       "id                                                                             \n",
       "1           1  2019-05-04  https://www.npr.org/templates/transcript/trans...   \n",
       "2           1  2019-05-04  https://www.npr.org/templates/transcript/trans...   \n",
       "3           1  2019-05-04  https://www.npr.org/templates/transcript/trans...   \n",
       "4           1  2019-05-04  https://www.npr.org/templates/transcript/trans...   \n",
       "5           1  2019-05-04  https://www.npr.org/templates/transcript/trans...   \n",
       "6           1  2019-05-04  https://www.npr.org/templates/transcript/trans...   \n",
       "7           1  2019-05-04  https://www.npr.org/templates/transcript/trans...   \n",
       "8           1  2019-05-04  https://www.npr.org/templates/transcript/trans...   \n",
       "9           2  2019-04-27  https://www.npr.org/templates/transcript/trans...   \n",
       "10          2  2019-04-27  https://www.npr.org/templates/transcript/trans...   \n",
       "\n",
       "        segment                                         transcript  train  \\\n",
       "id                                                                          \n",
       "1           who  \\n    \\n        \\n    \\n\\n    BILL KURTIS: Fro...   True   \n",
       "2         panel  \\n    \\n        \\n    \\n\\n    PETER SAGAL, HOS...   True   \n",
       "3         bluff  \\n    \\n        \\n    \\n\\n    BILL KURTIS: Fro...   True   \n",
       "4           job  \\n    \\n        \\n    \\n\\n    PETER SAGAL, HOS...   True   \n",
       "5         panel  \\n    \\n        \\n    \\n\\n    PETER SAGAL, HOS...  False   \n",
       "6      limerick  \\n    \\n        \\n    \\n\\n    PETER SAGAL, HOS...  False   \n",
       "7     lightning  \\n    \\n        \\n    \\n\\n    PETER SAGAL, HOS...  False   \n",
       "8   predictions  \\n    \\n        \\n    \\n\\n    PETER SAGAL, HOS...   True   \n",
       "9           who  \\n    \\n        \\n    \\n\\n    BILL KURTIS: Fro...   True   \n",
       "10        panel  \\n    \\n        \\n    \\n\\n    PETER SAGAL, HOS...   True   \n",
       "\n",
       "     test    val  \n",
       "id                \n",
       "1   False  False  \n",
       "2   False  False  \n",
       "3   False  False  \n",
       "4   False  False  \n",
       "5    True  False  \n",
       "6    True  False  \n",
       "7    True  False  \n",
       "8   False  False  \n",
       "9   False  False  \n",
       "10  False  False  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity, I'll set all letters to lower-case\n",
    "transcript_df.loc[:,'transcript'] = transcript_df.loc[:,'transcript'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Example transcript<a name=\"data-example\"></a>\n",
    "\n",
    "To understand the data, it helps to first see what the raw data looks like. Let's print a little bit of the transcript from the first dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    \n",
      "        \n",
      "    \n",
      "\n",
      "    bill kurtis: from npr and wbez chicago, this is wait wait... don't tell me, the npr news quiz. hey, arthur miller - step into this cruci-bill (ph).\n",
      "    (laughter)\n",
      "    kurtis: i'm bill kurtis. and here's your host at the chase bank auditorium in downtown chicago, peter sagal.\n",
      "    peter sagal, host: \n",
      "    thank you, bill. thank you, everybody.\n",
      "    (cheering)\n",
      "    sagal: thank you so much. we have a very interesting show for you today. later on, we're going to be talking to m\n"
     ]
    }
   ],
   "source": [
    "print(transcript_df.loc[1,'transcript'][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    \\n        \\n    \\n\\n    bill kurtis: from npr and wbez chicago, this is wait wait... don't tell me, the npr news quiz. hey, arthur miller - step into this cruci-bill (ph).\\n    (laughter)\\n    kurtis: i'm bill kurtis. and here's your host at the chase bank auditorium in downtown chicago, peter sagal.\\n    peter sagal, host: \\n    thank you, bill. thank you, everybody.\\n    (cheering)\\n    sagal: thank you so much. we have a very interesting show for you today. later on, we're going to be talking to microsoft co-founder steve ballmer. he is, we believe, the richest guest we've ever had. but, of course, your true wealth is measured in your friends. and this just in - he has more friends, too.\\n    (laughter)\\n    sagal: but first, as many of you know, the npr podcast feeds got all screwed up last week. people who tried to download our show got, for example, how i built this instead, for which i apologize. and the people who wanted how i built this got us, for which i apologize even more.\\n    (l\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_df.loc[1,'transcript'][:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, we can note a number of features. First, audience responses are noted with the '(LAUGHTER)' marker and '(APPLAUSE)' marker. This will prove very useful, as we have an automatic metric for \"funniness\" of the preceding text. \n",
    "\n",
    "Speakers' names are in all caps, followed by a colon. Speakers are also separated by a line break and a tab, which could potentially be used to segment the text into phrases by various people. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3 Encoding the text <a name='data-encoding'></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be building a letter-based generator for now, so I want a way to encode both letters and punctuation as integers (eventually, to be transferred into a one-hot encoding scheme for transferring to the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset includes 77 unique tokens\n"
     ]
    }
   ],
   "source": [
    "all_tokens = set(transcript_df.loc[transcript_df.train,'transcript'].str.cat())\n",
    "print(f'The dataset includes {len(all_tokens)} unique tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary converting letters/punctuation to integers\n",
    "conversion_dict = {}\n",
    "for i, token in enumerate(all_tokens):\n",
    "    conversion_dict[token] = i\n",
    "    \n",
    "# Make a second dictionary to go in the other direction\n",
    "reversion_dict = dict( (v,k) for k, v in conversion_dict.items() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple function to encode transcript\n",
    "def encode_transcript(transcript):\n",
    "    return [conversion_dict.get(n,len(all_tokens)) for n in transcript]\n",
    "def decode_transcript(transcript):\n",
    "    return [reversion_dict.get(n,len(all_tokens)) for n in transcript]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode each of the transcripts by converting letters and punctuation to integers\n",
    "transcript_df['encoded'] = transcript_df.loc[:,'transcript'].apply(encode_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>aired_at</th>\n",
       "      <th>url</th>\n",
       "      <th>segment</th>\n",
       "      <th>transcript</th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "      <th>val</th>\n",
       "      <th>encoded</th>\n",
       "      <th>encoded_words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-05-04</td>\n",
       "      <td>https://www.npr.org/templates/transcript/trans...</td>\n",
       "      <td>who</td>\n",
       "      <td>\\n    \\n        \\n    \\n\\n    bill kurtis: fro...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[3, 33, 33, 33, 33, 3, 33, 33, 33, 33, 33, 33,...</td>\n",
       "      <td>[88, 78, 39, 21, 6, 336, 161, 17, 11, 44, 44, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-05-04</td>\n",
       "      <td>https://www.npr.org/templates/transcript/trans...</td>\n",
       "      <td>panel</td>\n",
       "      <td>\\n    \\n        \\n    \\n\\n    peter sagal, hos...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[3, 33, 33, 33, 33, 3, 33, 33, 33, 33, 33, 33,...</td>\n",
       "      <td>[84, 2, 107, 23, 63, 317, 27, 102, 14, 7, 5, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-05-04</td>\n",
       "      <td>https://www.npr.org/templates/transcript/trans...</td>\n",
       "      <td>bluff</td>\n",
       "      <td>\\n    \\n        \\n    \\n\\n    bill kurtis: fro...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[3, 33, 33, 33, 33, 3, 33, 33, 33, 33, 33, 33,...</td>\n",
       "      <td>[88, 78, 39, 21, 6, 336, 161, 17, 11, 44, 44, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-05-04</td>\n",
       "      <td>https://www.npr.org/templates/transcript/trans...</td>\n",
       "      <td>job</td>\n",
       "      <td>\\n    \\n        \\n    \\n\\n    peter sagal, hos...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[3, 33, 33, 33, 33, 3, 33, 33, 33, 33, 33, 33,...</td>\n",
       "      <td>[84, 2, 107, 6, 63, 1, 119, 144, 3659, 81, 243...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-05-04</td>\n",
       "      <td>https://www.npr.org/templates/transcript/trans...</td>\n",
       "      <td>panel</td>\n",
       "      <td>\\n    \\n        \\n    \\n\\n    peter sagal, hos...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[3, 33, 33, 33, 33, 3, 33, 33, 33, 33, 33, 33,...</td>\n",
       "      <td>[84, 2, 107, 23, 63, 317, 117, 99, 255, 14, 7,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode_id    aired_at                                                url  \\\n",
       "id                                                                             \n",
       "1           1  2019-05-04  https://www.npr.org/templates/transcript/trans...   \n",
       "2           1  2019-05-04  https://www.npr.org/templates/transcript/trans...   \n",
       "3           1  2019-05-04  https://www.npr.org/templates/transcript/trans...   \n",
       "4           1  2019-05-04  https://www.npr.org/templates/transcript/trans...   \n",
       "5           1  2019-05-04  https://www.npr.org/templates/transcript/trans...   \n",
       "\n",
       "   segment                                         transcript  train   test  \\\n",
       "id                                                                            \n",
       "1      who  \\n    \\n        \\n    \\n\\n    bill kurtis: fro...   True  False   \n",
       "2    panel  \\n    \\n        \\n    \\n\\n    peter sagal, hos...   True  False   \n",
       "3    bluff  \\n    \\n        \\n    \\n\\n    bill kurtis: fro...   True  False   \n",
       "4      job  \\n    \\n        \\n    \\n\\n    peter sagal, hos...   True  False   \n",
       "5    panel  \\n    \\n        \\n    \\n\\n    peter sagal, hos...  False   True   \n",
       "\n",
       "      val                                            encoded  \\\n",
       "id                                                             \n",
       "1   False  [3, 33, 33, 33, 33, 3, 33, 33, 33, 33, 33, 33,...   \n",
       "2   False  [3, 33, 33, 33, 33, 3, 33, 33, 33, 33, 33, 33,...   \n",
       "3   False  [3, 33, 33, 33, 33, 3, 33, 33, 33, 33, 33, 33,...   \n",
       "4   False  [3, 33, 33, 33, 33, 3, 33, 33, 33, 33, 33, 33,...   \n",
       "5   False  [3, 33, 33, 33, 33, 3, 33, 33, 33, 33, 33, 33,...   \n",
       "\n",
       "                                        encoded_words  \n",
       "id                                                     \n",
       "1   [88, 78, 39, 21, 6, 336, 161, 17, 11, 44, 44, ...  \n",
       "2   [84, 2, 107, 23, 63, 317, 27, 102, 14, 7, 5, 2...  \n",
       "3   [88, 78, 39, 21, 6, 336, 161, 17, 11, 44, 44, ...  \n",
       "4   [84, 2, 107, 6, 63, 1, 119, 144, 3659, 81, 243...  \n",
       "5   [84, 2, 107, 23, 63, 317, 117, 99, 255, 14, 7,...  "
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features=None, categories='auto', drop=None,\n",
       "              dtype=<class 'numpy.float64'>, handle_unknown='error',\n",
       "              n_values=None, sparse=False)"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a one-hot encoder to finally yield data in a one-hot version\n",
    "onehotencoder = OneHotEncoder(categories='auto',sparse=False)\n",
    "onehotencoder.fit(np.concatenate(transcript_df.encoded.values).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to apply the one hot scheme to the integer-encoded values\n",
    "def one_hot_transcript(transcript):\n",
    "    integer_transcript = np.array(encode_transcript(transcript)).reshape(-1,1)\n",
    "    return onehotencoder.transform(integer_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the encoded transcript values to the one-hot encodings\n",
    "transcript_df['encoded'] = transcript_df.loc[:,'transcript'].apply(one_hot_transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.5 Data Generator\n",
    "This is a different way of making the data. Doing the massive conversion of transcript to one-hot encoding is pretty memory-intensive. One way to improve that is to only process some of the data at a time, using a generator. This could also enable me to add some additional randomization into my process, so I'm going to play around with it. This code is closely based on the tutorial here: https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "n_times = 100\n",
    "n_components = len(all_tokens)+1\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    # Generates data for Keras\n",
    "    def __init__(self, df, batch_size=batch_size,n_times=n_times,n_components=n_components,shuffle=True):\n",
    "        # Initialization\n",
    "        self.batch_size = batch_size\n",
    "        self.df = df.encoded\n",
    "        self.indices = np.copy(self.df.index.values)\n",
    "        self.dim = (batch_size,n_times,n_components)\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        # Denotes the number of batches per epoch\n",
    "        return len(self.df.index.values)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(self.indices[index])\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def __data_generation(self, transcript_num):\n",
    "        #'Generates data containing batch_size samples' \n",
    "        # Initialization\n",
    "        X = np.empty(self.dim)\n",
    "        y = np.empty((self.batch_size,self.dim[-1]), dtype=int)\n",
    "\n",
    "        # Pull batch_size random samples from the specified transcript\n",
    "        indices = np.random.randint(low=0,\n",
    "                                    high=self.df[transcript_num].shape[0]-n_times-1,\n",
    "                                    size=self.batch_size)\n",
    "        \n",
    "        # for each start position, pull a sequence of n_times chars for X, and the next char for y\n",
    "        for i, startpos in enumerate(indices):\n",
    "            # Store sample\n",
    "            X[i,] = self.df[transcript_num][startpos:startpos+n_times,:]\n",
    "\n",
    "            # Store class\n",
    "            y[i] = self.df[transcript_num][startpos+n_times,:]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = DataGenerator(transcript_df.loc[transcript_df.train,:],shuffle=False)\n",
    "val_gen = DataGenerator(transcript_df.loc[transcript_df.val,:],shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x6f602a160>]"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_gen.__getitem__(0)[0][0].sum(axis=0))\n",
    "plt.plot(train_gen.__getitem__(0)[0][0].sum(axis=0))\n",
    "plt.plot(train_gen.__getitem__(0)[0][0].sum(axis=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Markov Chain <a name=\"markov\"></a>\n",
    "As a control, let's build a very simple system, called a \"Markov chain.\" This system simply calculates the probability of a next character, given the previous characters (how many previous characters is a selectable parameter). It then generates text by sampling this probability matrix. We can compare the performance of the Markov chain with the performance of the neural networks we build later.\n",
    "\n",
    "This code initialized with the tutorial provided here: https://eli.thegreenplace.net/2018/elegant-python-code-for-a-markov-chain-text-generator/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "\n",
    "# This is the length of the \"state\" the current character is predicted from.\n",
    "STATE_LEN = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join all transcripts together\n",
    "data = transcript_df.loc[:,'transcript'].str.cat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning model...\n",
      "0/31012208\n",
      "10000000/31012208\n",
      "20000000/31012208\n",
      "30000000/31012208\n"
     ]
    }
   ],
   "source": [
    "# initialize model\n",
    "markovmodel = defaultdict(Counter)\n",
    "print('Learning model...')\n",
    "for i in range(len(data) - STATE_LEN):\n",
    "    state = data[i:i + STATE_LEN]\n",
    "    nxt = data[i + STATE_LEN]\n",
    "    markovmodel[state][nxt] += 1\n",
    "    if i%10000000 ==0:\n",
    "        print(f'{i}/{len(data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling...\n",
      " mr. burbank: because, like, the end of the world have some special feature - the low income tenants will get their very own.\n",
      "    (laughter)\n",
      "    sagal: ...of the squirrel in the motor. \n",
      "    sagal: right.\n",
      "    (soundbite of laughter) \n",
      "    mr. bodett: you know, the saddle is throwing you and stuff...\n",
      "    poundstone: there you go.\n",
      "    sagal: i know, mo, i've said this before on this program for kids.\n",
      "    (soundbite of laughter)\n",
      "    (soundbite of bell)\n",
      "    sagal: that was john boehner said he was a monk.\"\n",
      "    (laughter)\n",
      "    bodden: her tv. no idea.\n",
      "    sagal: right.\n",
      "    (soundbite of bell)\n",
      "    sagal: it's true.\n",
      "    (soundbite of laughter)\n",
      "    m: because he wasn't talking to the super bowl, the denver broncos, washington state used a bow and arrow. scientists tested this milk substance, and they found him behind the wheel. so you can blame it for anything. it's not like a state occasion or an official occasion. like he pays for his kids' rice crispies, that's, like, america's youth. they weren't, you know,\n"
     ]
    }
   ],
   "source": [
    "print('Sampling...')\n",
    "state = random.choice(list(markovmodel))\n",
    "out = state\n",
    "for i in range(1000):\n",
    "    out += random.choices(list(markovmodel[state]), markovmodel[state].values())[0]\n",
    "    state = out[-STATE_LEN:]\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Chain conclusions\n",
    "Because the Markov chain samples from the distribution of previously-seen phrases, everything that it says sounds like language. Phrases even make sense for short durations, but rarely over long ones, due to the fact that it only uses a set amount of past characters to generate future characters. So it does an OK job, but certainly doesn't make anything that truly sounds like language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Recurrent Neural Network Model <a name='rnn-initialize'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate transcript function\n",
    "First, specify a function that, given a model and a sample of text, will generate the next text for the transcript. We'll run this function occasionally in order to visualize how our model learning is progressing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predict_transcript(model,generator,chars_to_generate=50):\n",
    "    sample = generator.__getitem__(np.random.randint(500))[0][np.random.randint(batch_size)]\n",
    "    sample = sample.reshape(1,sample.shape[0],sample.shape[1])\n",
    "    \n",
    "    # initialize the phrase\n",
    "    predicted_phrase = np.concatenate((sample,np.zeros((1,chars_to_generate,n_components))),axis=1)\n",
    "\n",
    "    for char in range(n_times,n_times+chars_to_generate):\n",
    "        # calculate the probability distribution\n",
    "        probability = model.predict(sample).squeeze()\n",
    "\n",
    "        # guess the next character and add it to our prediction\n",
    "        guess = np.random.choice(a=n_components,p=probability)\n",
    "        predicted_phrase[0,char,guess] = 1\n",
    "\n",
    "        # feed the results into the next step of the model\n",
    "        sample = predicted_phrase[:,(char+1-n_times):char+1,:]\n",
    "\n",
    "    # Convert to text\n",
    "    predicted_phrase = ''.join(decode_transcript(np.argmax(predicted_phrase,axis=-1).squeeze()))\n",
    "    \n",
    "    # output the phrase\n",
    "    return predicted_phrase\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to use a recurrent neural network (GRU) to generate my text. There are other systems I would also like to play with in the future (e.g. Markov models, transformer network), but this will make a good start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential models are pretty simple, I'll start there.\n",
    "from keras.models import Model\n",
    "from keras.layers import GRU,Dense,Bidirectional,Input,Add,Dropout,SpatialDropout1D,GlobalAveragePooling1D,BatchNormalization\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import Adam,Nadam\n",
    "\n",
    "# Later on I'm going to try out some attention\n",
    "# for speed at the moment, I'll use attention layers that have already been written\n",
    "from keras_self_attention import SeqSelfAttention # source: https://github.com/CyberZHG/keras-self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter setting\n",
    "n_units = 128 # size of RNN layers\n",
    "\n",
    "# helper function to add in a middle GRU layer and sum the outputs+inputs to that layer\n",
    "def Add_GRU_layer(inp,n_units):\n",
    "    mid = Bidirectional(GRU(n_units,activation='tanh',input_shape=(n_times,n_components),return_sequences=True))(inp)\n",
    "    mid = SeqSelfAttention(attention_activation='sigmoid')(mid)\n",
    "    final = Add()([mid,inp])\n",
    "    final = BatchNormalization()(final)\n",
    "    return final\n",
    "\n",
    "# helper function to incorporate attention\n",
    "\n",
    "## Model layer definitions start here\n",
    "x_in = Input(shape=(n_times,n_components))\n",
    "\n",
    "# GRU layer\n",
    "x = Bidirectional(GRU(n_units,activation='tanh',input_shape=(n_times,n_components),return_sequences=True))(x_in)\n",
    "x = SeqSelfAttention(attention_activation='sigmoid')(x)\n",
    "\n",
    "# middle layers, with skip-gram combinations & attention\n",
    "for mid_layer in range(1):\n",
    "    x = Add_GRU_layer(x,n_units)\n",
    "\n",
    "# final layer, average over time\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dropout(rate=0.25)(x)\n",
    "x = Dense(128,activation='relu')(x)\n",
    "x = Dropout(rate=0.25)(x)\n",
    "dense_out = Dense(n_components,activation='softmax')(x)\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop()\n",
    "model = Model(inputs=[x_in],outputs=[dense_out])\n",
    "model.compile(loss='categorical_crossentropy',optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_39\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_29 (InputLayer)        (None, 10, 78)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_13 (Bidirectio (None, 10, 256)           158976    \n",
      "_________________________________________________________________\n",
      "seq_self_attention_13 (SeqSe (None, 10, 256)           16449     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_51  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_230 (Dense)            (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_231 (Dense)            (None, 78)                10062     \n",
      "=================================================================\n",
      "Total params: 218,383\n",
      "Trainable params: 218,383\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Generate a model summary to visualize the layers and model size\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Train the model<a name='rnn-train'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "2639/2639 [==============================] - 904s 342ms/step - loss: 2.4362 - val_loss: 2.2565\n",
      "Epoch 2/3\n",
      "2639/2639 [==============================] - 906s 343ms/step - loss: 2.3492 - val_loss: 2.1829\n",
      "Epoch 3/3\n",
      "2639/2639 [==============================] - 877s 332ms/step - loss: 2.3003 - val_loss: 2.1667\n",
      "r news quiz. i'm bill kurtis. and here is your host at the chase bank auditorium in downtown chicagoc rogpet lirram? fairi.. od, lodginrre. tharode kat ghoteridgaen leiwrirylisbaatam ak\n",
      "    het\n",
      "    re\n",
      "Epoch 1/3\n",
      "2639/2639 [==============================] - 926s 351ms/step - loss: 2.2334 - val_loss: 2.0935\n",
      "Epoch 2/3\n",
      "2639/2639 [==============================] - 879s 333ms/step - loss: 2.2271 - val_loss: 2.0714\n",
      "Epoch 3/3\n",
      "2639/2639 [==============================] - 847s 321ms/step - loss: 2.2109 - val_loss: 2.0501\n",
      "nd an owl makes and nothing else.\n",
      "    goldthwait: so it's hooters.\n",
      "    sagal: but for...\n",
      "    goldthwh.ecis. rei pifisad: oi a: fagats a'nrt tond that.. idly..\n",
      "    parpares ehigo es to roem chitim. seu\n",
      "Epoch 1/3\n",
      "1695/2639 [==================>...........] - ETA: 4:47 - loss: 2.1904"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-537-7a824e047f5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                                   \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                                   \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                                   callbacks = [early_stopping])\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPredict_transcript\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchars_to_generate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/BimanualModeling/py_36_env/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/BimanualModeling/py_36_env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/BimanualModeling/py_36_env/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    214\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/BimanualModeling/py_36_env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/BimanualModeling/py_36_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/BimanualModeling/py_36_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/BimanualModeling/py_36_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss',patience=1,restore_best_weights=True)\n",
    "n_epochs = 3\n",
    "decay_rate = 1/(sum(transcript_df.train)*n_epochs)\n",
    "\n",
    "for lr in [0.01,0.005,0.001,0.0005,0.0001]:\n",
    "    optimizer=keras.optimizers.RMSprop(lr=lr,decay=decay_rate)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=optimizer)\n",
    "    history = model.fit_generator(train_gen,\n",
    "                                  epochs=n_epochs,\n",
    "                                  validation_data = val_gen,\n",
    "                                  validation_steps = sum(transcript_df.val),\n",
    "                                  steps_per_epoch=sum(transcript_df.train),\n",
    "                                  callbacks = [early_stopping])\n",
    "    print(Predict_transcript(model,val_gen,chars_to_generate=100));\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './GRU_Text_Generator2'\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(model_path, custom_objects=SeqSelfAttention.get_custom_objects())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Assess performance<a name='rnn-test'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one: how will they know? i mean, don't you just buy a ticket and go in?\n",
      "    (laughter)\n",
      "    bodden: he'll. allny, fonts, amores the ars the, couldersfar blanidiesns?\n",
      "    sagal: po amber do e it's lay w\n"
     ]
    }
   ],
   "source": [
    "print(Predict_transcript(model,val_gen,chars_to_generate=100));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network is doing ok, but nothing to write home about at the moment. We get a couple of \"real\" words, and the included punctuation looks reasonable. However, the final result is not recognizable English. This suggests to me that the network likely needs more training time, or possibly more units or more training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention network <a name='attn-initialize'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras_multi_head import MultiHeadAttention,MultiHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_18\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input (InputLayer)              (None, 100, 78)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_12 (TimeDistri (None, 100, 128)     10112       Input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_55 (SpatialDr (None, 100, 128)     0           time_distributed_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_54 (MultiH (None, 100, 128)     66048       spatial_dropout1d_55[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 100, 128)     0           spatial_dropout1d_55[0][0]       \n",
      "                                                                 multi_head_attention_54[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 100, 128)     512         add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_93 (Dense)                (None, 100, 128)     16512       batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 100, 128)     512         dense_93[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_56 (SpatialDr (None, 100, 128)     0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_55 (MultiH (None, 100, 128)     66048       spatial_dropout1d_56[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 100, 128)     0           spatial_dropout1d_55[0][0]       \n",
      "                                                                 multi_head_attention_55[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 100, 128)     512         add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_94 (Dense)                (None, 100, 128)     16512       batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 100, 128)     512         dense_94[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_37 (Gl (None, 128)          0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_35 (Dropout)            (None, 128)          0           global_average_pooling1d_37[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dense_95 (Dense)                (None, 78)           10062       dropout_35[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 187,342\n",
      "Trainable params: 186,318\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "layer_size = 128\n",
    "\n",
    "# Function to add a set of layers that together make up a \"transformer\"\n",
    "def Add_Transformer(inp_layer,head_num=16):\n",
    "    att_layer = MultiHeadAttention(\n",
    "        head_num=head_num,\n",
    "        activation='sigmoid',\n",
    "    )(inp_layer)\n",
    "\n",
    "    summed = keras.layers.Add()([embedded,att_layer])\n",
    "    normed = BatchNormalization()(summed)\n",
    "    \n",
    "    # follow each attention layer with a feed-forward layer\n",
    "    feed_forward = Dense(layer_size)(normed)\n",
    "    normed = BatchNormalization()(feed_forward)\n",
    "    \n",
    "    return normed\n",
    "\n",
    "# Start building layers\n",
    "input_layer = keras.layers.Input(\n",
    "    shape=(n_times, n_components),\n",
    "    name='Input',\n",
    ")\n",
    "embedded = keras.layers.TimeDistributed(Dense(layer_size))(input_layer)\n",
    "embedded = SpatialDropout1D(rate=0.25)(embedded)\n",
    "\n",
    "# Transformer 1\n",
    "x = Add_Transformer(embedded)\n",
    "x = SpatialDropout1D(rate=0.25)(x)\n",
    "\n",
    "# Transformer layer 2\n",
    "for layer in range(1):\n",
    "    x = Add_Transformer(x)\n",
    "\n",
    "# readout\n",
    "flattened = keras.layers.GlobalAveragePooling1D()(x)\n",
    "flattened = Dropout(rate=0.25)(flattened)\n",
    "dense_out = Dense(n_components,activation='softmax')(flattened)\n",
    "\n",
    "model2 = keras.models.Model(inputs=input_layer, outputs=dense_out)\n",
    "model2.compile(\n",
    "    optimizer='nadam',\n",
    "    loss='categorical_crossentropy',\n",
    ")\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Train the attention model <a name='attn-train'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2639/2639 [==============================] - 737s 279ms/step - loss: 3.0500 - val_loss: 2.9940\n",
      "Epoch 2/100\n",
      " 506/2639 [====>.........................] - ETA: 9:10 - loss: 3.0062"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-281-8cdf078fb490>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                               \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                               callbacks = [early_stopping])\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/BimanualModeling/py_36_env/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/BimanualModeling/py_36_env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/BimanualModeling/py_36_env/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    214\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/BimanualModeling/py_36_env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/BimanualModeling/py_36_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/BimanualModeling/py_36_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/BimanualModeling/py_36_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss',patience=1,restore_best_weights=True)\n",
    "history = model2.fit_generator(train_gen,\n",
    "                              epochs=100,\n",
    "                              validation_data = val_gen,\n",
    "                              validation_steps = sum(transcript_df.val),\n",
    "                              steps_per_epoch=sum(transcript_df.train),\n",
    "                              callbacks = [early_stopping])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Assess attention model performance <a name='attn-test'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.evaluate_generator(val_gen)\n",
    "print(Predict_transcript(model2, val_gen,chars_to_generate=100));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Word-based model\n",
    "Looks like the letter-based models are pretty hard to get accurate, especially given that at the moment I am only running on a CPU. I wonder if a word-based model would perform better, given that the thorny \"first learn English\" problem is less of a worry. To do this, I'll need to process the inputs differently. Let's see what I can do.\n",
    "\n",
    "**Edit**: I realized a subtle bug that I had in my character training/validation generator function above, such that selections were being repeated from epoch to epoch. I have fixed it, and will go back soon and see if things improve once I'm actually training on more of my training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Processing Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to \"tokenize\" the input, e.g. convert words into numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the tokenizer class, specify vocabulary size\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "n_words = 10000\n",
    "\n",
    "# build and train a tokenizer instance\n",
    "tokenizer = Tokenizer(num_words=n_words)\n",
    "tokenizer.fit_on_texts(transcript_df.transcript)\n",
    "\n",
    "# tokenize our texts\n",
    "transcript_df['encoded_words'] = tokenizer.texts_to_sequences(transcript_df.transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that we don't have to re-invent the wheel, I'll also use GLOVE embeddings to incorporate some fore-knowledge of word similarities into my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = './'\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\n",
    "EMBEDDING_DIM = 50\n",
    "\n",
    "# Make the embedding matrix\n",
    "def Make_embedding_matrix(tokenizer):\n",
    "    word_index = tokenizer.word_index\n",
    "    # Create a dictionary mapping words to GLOVE embeddings\n",
    "    print('Indexing word vectors.')\n",
    "    embeddings_index = {}\n",
    "    with open(os.path.join(GLOVE_DIR, f'glove.6B.{EMBEDDING_DIM}d.txt')) as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    print(f'Found {len(embeddings_index)} word vectors.')\n",
    "    \n",
    "    # Prepare embedding matrix\n",
    "    num_words = len(word_index) + 1\n",
    "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        if i > n_words:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = Make_embedding_matrix(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a generator\n",
    "As before, we'll want to make a generator to make data to feed our networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator_words(keras.utils.Sequence):\n",
    "    # Generates data for Keras\n",
    "    def __init__(self, df, batch_size=batch_size,n_times=n_times,n_components=n_words,shuffle=True):\n",
    "        # Initialization\n",
    "        self.batch_size = batch_size\n",
    "        self.df = df.encoded_words\n",
    "        self.indices = np.copy(self.df.index.values)\n",
    "        self.dim = (batch_size,n_times)\n",
    "        self.shuffle = shuffle\n",
    "        self.n_components = n_components\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        # Denotes the number of batches per epoch\n",
    "        return len(self.df.index.values)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(self.indices[index])\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def __data_generation(self, transcript_num):\n",
    "        #'Generates data containing batch_size samples' \n",
    "        \n",
    "        \n",
    "        # timing signal so that the network can figure out how to use longer times\n",
    "        timing_signal = np.sin(np.repeat(np.arange(n_times).reshape(-1,1),10,axis=1)/np.arange(1,11)).reshape(1,n_times,10)\n",
    "        timing_signal = np.repeat(timing_signal,batch_size,axis=0)\n",
    "        # start from last word, so that encoding for later words is consistent as duration increases\n",
    "        timing_signal = timing_signal[:,::-1,:] \n",
    "        \n",
    "        # Initialization\n",
    "        X = np.empty(self.dim)\n",
    "        y = np.zeros((self.batch_size,self.n_components), dtype=int)\n",
    "\n",
    "        # Pull batch_size random samples from the specified transcript\n",
    "        indices = np.random.randint(low=0,\n",
    "                                    high=len(self.df[transcript_num])-n_times-1,\n",
    "                                    size=self.batch_size)\n",
    "        \n",
    "        # for each start position, pull a sequence of n_times chars for X, and the next char for y\n",
    "        for i, startpos in enumerate(indices):\n",
    "            # Store sample\n",
    "            X[i,] = self.df[transcript_num][startpos:startpos+n_times]\n",
    "\n",
    "            # Store next word\n",
    "            y[i, self.df[transcript_num][startpos+n_times]] = 1 # one-hot encoding\n",
    "        \n",
    "        X = [X,timing_signal]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "n_times=10\n",
    "train_gen = DataGenerator_words(transcript_df.loc[transcript_df.train,:],shuffle=True,n_times=n_times)\n",
    "val_gen = DataGenerator_words(transcript_df.loc[transcript_df.val,:],shuffle=True,n_times=n_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54080, 50)"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_38\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Words (InputLayer)              (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_77 (Embedding)        (None, 10, 50)       2704000     Words[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_225 (Dense)               (None, 10, 502)      25602       embedding_77[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "times (InputLayer)              (None, 10, 10)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 10, 512)      0           dense_225[0][0]                  \n",
      "                                                                 times[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_104 (Multi (None, 10, 512)      1050624     concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_97 (Add)                    (None, 10, 512)      0           concatenate_13[0][0]             \n",
      "                                                                 multi_head_attention_104[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_83 (LayerNo (None, 10, 512)      1024        add_97[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_226 (Dense)               (None, 10, 512)      262656      layer_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_98 (Add)                    (None, 10, 512)      0           dense_226[0][0]                  \n",
      "                                                                 layer_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_84 (LayerNo (None, 10, 512)      1024        add_98[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_71 (SpatialDr (None, 10, 512)      0           layer_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_105 (Multi (None, 10, 512)      1050624     spatial_dropout1d_71[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_99 (Add)                    (None, 10, 512)      0           spatial_dropout1d_71[0][0]       \n",
      "                                                                 multi_head_attention_105[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_85 (LayerNo (None, 10, 512)      1024        add_99[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_227 (Dense)               (None, 10, 512)      262656      layer_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_100 (Add)                   (None, 10, 512)      0           dense_227[0][0]                  \n",
      "                                                                 layer_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_86 (LayerNo (None, 10, 512)      1024        add_100[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_72 (SpatialDr (None, 10, 512)      0           layer_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)            (None, 5120)         0           spatial_dropout1d_72[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_228 (Dense)               (None, 512)          2621952     flatten_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_229 (Dense)               (None, 10000)        5130000     dense_228[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 13,112,210\n",
      "Trainable params: 10,408,210\n",
      "Non-trainable params: 2,704,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.initializers import Constant\n",
    "from keras_layer_normalization import LayerNormalization\n",
    "\n",
    "layer_size = 512\n",
    "head_num = 8\n",
    "\n",
    "sequence_input = Input(shape=(n_times,),\n",
    "                           dtype='int32',name='Words')\n",
    "timing_input = Input(shape=(n_times,10),name='times')\n",
    "\n",
    "# embedded = Embedding(embedding_matrix.shape[0],\n",
    "#                             embedding_matrix.shape[1],\n",
    "# #                             embeddings_initializer=Constant(embedding_matrix),\n",
    "#                             input_length=None,\n",
    "#                             trainable=False)(sequence_input)\n",
    "embedded = Embedding(n_words,\n",
    "                            embedding_matrix.shape[1],\n",
    "#                             embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=None,\n",
    "                            trainable=False)(sequence_input)\n",
    " = Dense()(embedded)\n",
    "\n",
    "# add in a timing signal\n",
    "x = keras.layers.Concatenate()([x,timing_input])\n",
    "\n",
    "# Multi-head attention layer\n",
    "attn = MultiHeadAttention(head_num=head_num,activation='sigmoid')(x)\n",
    "summed = Add()([x,attn])\n",
    "normed = LayerNormalization()(summed)\n",
    "\n",
    "# Feed-forward layer\n",
    "x = Dense(layer_size,activation='relu')(normed)\n",
    "summed = Add()([x,normed])\n",
    "normed = LayerNormalization()(summed)\n",
    "normed = SpatialDropout1D(rate=0.25)(normed)\n",
    "\n",
    "# Multi-head attention layer\n",
    "for mid_layers in range(1):\n",
    "    attn = MultiHeadAttention(head_num=head_num,activation='sigmoid')(normed)\n",
    "    summed = Add()([normed,attn])\n",
    "    normed = LayerNormalization()(summed)\n",
    "\n",
    "    # Feed-forward layer\n",
    "    x = Dense(layer_size,activation='relu')(normed)\n",
    "    summed = Add()([x,normed])\n",
    "    normed = LayerNormalization()(summed)\n",
    "    normed = SpatialDropout1D(rate=0.25)(normed)\n",
    "\n",
    "# Final readout\n",
    "pooled = keras.layers.Flatten()(normed)\n",
    "linear = Dense(layer_size,activation='relu')(pooled)\n",
    "readout = Dense(n_words,activation='softmax')(linear)\n",
    "\n",
    "\n",
    "# compile the model\n",
    "optimizer = keras.optimizers.RMSprop(decay=0.001)\n",
    "model = Model(inputs=[sequence_input,timing_input],outputs = readout)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predict_words(model,val_gen,n_words_to_predict=10):\n",
    "    inp = val_gen.__getitem__(np.random.randint(sum(transcript_df.val)))[0][0]\n",
    "    predicted_text = inp[np.random.randint(batch_size)].reshape(1,-1)\n",
    "    for word in range(n_words_to_predict):\n",
    "        predicted_token = np.random.choice(a=n_words,size=1,p=model2.predict(predicted_text)[0]).reshape(1,1)\n",
    "        predicted_text = np.concatenate((predicted_text,predicted_token),axis=-1)\n",
    "    print(tokenizer.sequences_to_texts(predicted_text)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionCallback(keras.callbacks.Callback):\n",
    "    \n",
    "    def on_epoch_end(x,y,z):\n",
    "        Predict_words(model,val_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1589/2639 [=================>............] - ETA: 3:54 - loss: 5.8593"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-494-0d9ef8832c6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                               \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                               callbacks = [early_stopping,PredictionCallback()])\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/BimanualModeling/py_36_env/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/BimanualModeling/py_36_env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/BimanualModeling/py_36_env/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    214\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/BimanualModeling/py_36_env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/BimanualModeling/py_36_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/BimanualModeling/py_36_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/BimanualModeling/py_36_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# progressively learn longer strings\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss',patience=2,restore_best_weights=True)\n",
    "batch_size = 64\n",
    "\n",
    "for n_times in range(10,100,10):\n",
    "    train_gen = DataGenerator_words(transcript_df.loc[transcript_df.train,:],shuffle=True,n_times=n_times,batch_size=batch_size)\n",
    "    val_gen = DataGenerator_words(transcript_df.loc[transcript_df.val,:],shuffle=True,n_times=n_times,batch_size=batch_size)\n",
    "    \n",
    "    history = model.fit_generator(train_gen,\n",
    "                              epochs=5,\n",
    "                              validation_data = val_gen,\n",
    "                              validation_steps = sum(transcript_df.val),\n",
    "                              steps_per_epoch=sum(transcript_df.train),\n",
    "                              callbacks = [early_stopping,PredictionCallback()])\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o'clock you got him yep i got him i'm following wait wait he's a white guy he's a white guy hold fire stand down peaceful laughter jobrani tell it like it is my brother sagal i know jobrani as the iranian i appreciate you saying that laughter salie no this these are is pair soundbite wondered a go soundbite their\n"
     ]
    }
   ],
   "source": [
    "Predict_words(model,val_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 20)"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timing_signal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_36_env",
   "language": "python",
   "name": "py_36_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
