{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to implement GLOVE embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import mysql.connector\n",
    "import re\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# change the default font size in figures to be larger\n",
    "font = {'size'   : 15}\n",
    "\n",
    "plt.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the database of wait wait don't tell me transcripts\n",
    "cnx = mysql.connector.connect(database='wait_wait',\n",
    "                              user='root')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to pull some transcripts from the database\n",
    "def pull_transcript(n=5):\n",
    "    # instantiate a cursor to select data from the database\n",
    "    curs = cnx.cursor()\n",
    "    curs.execute(f'select * from transcripts limit {n}')\n",
    "    \n",
    "    # pull the data and convert to a pandas dataframe\n",
    "    df = pd.DataFrame(data = np.array(curs.fetchmany(n)),columns=curs.column_names)\n",
    "    df = df.set_index('id')\n",
    "    \n",
    "    # close the cursor\n",
    "    curs.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_transcripts = 4000\n",
    "transcript_df = pull_transcript(n=num_transcripts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to go ahead and split the tables into testing and training sets, so that we don't over-fit. \n",
    "np.random.seed(42)\n",
    "transcript_df['train'] = np.random.rand(num_transcripts)>.2\n",
    "\n",
    "# divide into two data structures\n",
    "test_transcript_df = transcript_df.loc[transcript_df.train==False,:]\n",
    "train_transcript_df = transcript_df.loc[transcript_df.train,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_info(transcript_numbers):\n",
    "    \n",
    "    # extract the transcript, and divide it by lines\n",
    "    transcript = transcript_df.loc[transcript_numbers,'transcript'].str.cat();\n",
    "    by_lines = transcript.split('\\n    ')\n",
    "    df = pd.DataFrame(by_lines,columns=['lines']) # turn the lines into a dataframe\n",
    "    \n",
    "    # if the line contains \"LAUGHTER\", mark the previous line as \"funny\"\n",
    "    df['funny'] = df['lines'].str.contains('LAUGHTER')\n",
    "    df['funny'] = df.funny.shift(-1) # need funny to modify the previous line\n",
    "    \n",
    "    # if the line contains \"APPLAUSE\", mark the previous line as \"clapping\"\n",
    "    df['clapping'] = df['lines'].str.contains('APPLAUSE')\n",
    "    df['clapping'] = df['lines'].str.contains('CLAPPING')\n",
    "    df['clapping'] = df['lines'].str.contains('CHEERING')\n",
    "    df['clapping'] = df.clapping.shift(-1)\n",
    "\n",
    "    # for the remaining lines, identify the speaker using regular expressions\n",
    "    df['speaker'] = df['lines'].str.extract(r'([A-Z]+:)',expand=False).str.replace(r':','')\n",
    "    df['lines'] = df['lines'].str.replace(r'.+?(?=:)','').str.replace(r':','')\n",
    "    df['speaker'] = df['speaker'].str.replace('JR','BLOUNT') # disambiguating Roy Blount Jr\n",
    "    df['speaker'] = df['speaker'].str.replace('HOST','SAGAL') # sometimes refers to Sagal as host\n",
    "    # NOTE: There's some weird formatting with Peter Sagal's first line, \n",
    "    # where it calls him \"Host\" and puts his statement on the next line. It's not a huge thing,\n",
    "    # so I'm leaving it for now, but might be worth coming back to later.\n",
    "    \n",
    "    # drop the lines with no speaker (Applause markers, empty lines, etc.)\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Create a column with the number of separated words\n",
    "    df['num_words'] = df.lines.str.split().str.len()\n",
    "    \n",
    "    # Normalize the number of words with a quantile transformer\n",
    "    QT = QuantileTransformer(n_quantiles=1000,output_distribution='uniform')\n",
    "    QT.fit(df.num_words.values.reshape(-1,1)) \n",
    "    df['uniform_words'] = QT.transform(df.num_words.values.reshape(-1,1))\n",
    "    \n",
    "    # Create a column with whether the previous line was funny (for predictions)\n",
    "    df['prev_line_funny'] = df.funny.shift(1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lines</th>\n",
       "      <th>funny</th>\n",
       "      <th>clapping</th>\n",
       "      <th>speaker</th>\n",
       "      <th>num_words</th>\n",
       "      <th>uniform_words</th>\n",
       "      <th>prev_line_funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From NPR and WBEZ Chicago, this is WAIT WAIT....</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>KURTIS</td>\n",
       "      <td>25</td>\n",
       "      <td>8.358358e-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I'm Bill Kurtis. And here's your host at the ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>KURTIS</td>\n",
       "      <td>17</td>\n",
       "      <td>7.347347e-01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>SAGAL</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Thank you so much. We have a very interesting...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>SAGAL</td>\n",
       "      <td>56</td>\n",
       "      <td>9.654655e-01</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>But first, as many of you know, the NPR podca...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>SAGAL</td>\n",
       "      <td>53</td>\n",
       "      <td>9.604605e-01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                lines  funny clapping speaker  \\\n",
       "4    From NPR and WBEZ Chicago, this is WAIT WAIT....   True    False  KURTIS   \n",
       "6    I'm Bill Kurtis. And here's your host at the ...  False    False  KURTIS   \n",
       "7                                                      False    False   SAGAL   \n",
       "10   Thank you so much. We have a very interesting...   True    False   SAGAL   \n",
       "12   But first, as many of you know, the NPR podca...   True    False   SAGAL   \n",
       "\n",
       "    num_words  uniform_words prev_line_funny  \n",
       "4          25   8.358358e-01             NaN  \n",
       "6          17   7.347347e-01            True  \n",
       "7           0   1.000000e-07           False  \n",
       "10         56   9.654655e-01           False  \n",
       "12         53   9.604605e-01            True  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process the training set and show the top of the dataframe\n",
    "lines_df = line_info(train_transcript_df.index)\n",
    "lines_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant\n",
    "\n",
    "\n",
    "BASE_DIR = ''\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\n",
    "TEXT_DATA_DIR = os.path.join(BASE_DIR, '20_newsgroup')\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# second, prepare text samples and their labels\n",
    "print('Processing text dataset')\n",
    "\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    if os.path.isdir(path):\n",
    "        label_id = len(labels_index)\n",
    "        labels_index[name] = label_id\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            if fname.isdigit():\n",
    "                fpath = os.path.join(path, fname)\n",
    "                args = {} if sys.version_info < (3,) else {'encoding': 'latin-1'}\n",
    "                with open(fpath, **args) as f:\n",
    "                    t = f.read()\n",
    "                    i = t.find('\\n\\n')  # skip header\n",
    "                    if 0 < i:\n",
    "                        t = t[i:]\n",
    "                    texts.append(t)\n",
    "                labels.append(label_id)\n",
    "\n",
    "print('Found %s texts.' % len(texts))\n",
    "\n",
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "\n",
    "print('Preparing embedding matrix.')\n",
    "\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "print('Training model.')\n",
    "\n",
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/reuters.npz\n",
      "2113536/2110848 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import reuters\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(path=\"reuters.npz\",\n",
    "                                                         num_words=None,\n",
    "                                                         skip_top=0,\n",
    "                                                         maxlen=None,\n",
    "                                                         test_split=0.2,\n",
    "                                                         seed=113,\n",
    "                                                         start_char=1,\n",
    "                                                         oov_char=2,\n",
    "                                                         index_from=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  4,  3,  4,  4,  4,  4,  3,  3, 16])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_36_env",
   "language": "python",
   "name": "py_36_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
